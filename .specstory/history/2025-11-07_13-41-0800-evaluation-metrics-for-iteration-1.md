<!-- Generated by SpecStory, Markdown v2.0.0 -->

<!-- cursor Session 31129595-cfbe-4bcf-8539-1bbe3d39b1ae (2025-11-07 13:41-0800) -->

# Evaluation metrics for iteration 1 (2025-11-07 13:41-0800)

_**User (2025-11-06 15:12-0800)**_

@Evaluation Metrics.md @Iteration 1 -Arch_Impl_v2.md @Iteration 1 -PipelineDesign-v2.md @pipeline.py 



Looking at the docs attached and the way EValuation metrics are to be designed, what are the overall Evaluation metrics that would be needed and relevant to Iteration 1? 

---

_**Agent (model default, mode Agent)**_

<think><details><summary>Thought Process</summary>
**Summarizing evaluation metrics focus**</details></think>

---

## Iteration‚Äë1 Evaluation Metrics

**Response Quality**
- `Exact Match / Contains`: verify JSON schema fields, company name, citation URLs exactly match expected formats.
- `Regex validation`: enforce URL/domain formats (`a16z.com`), required quote syntax (`"VC Criterion:"`, `"Business Plan:"`).
- `BLEU/ROUGE & BERTScore`: spot-check short explanations against curated references to catch drift in claim wording.

**Retrieval Quality**
- `Context Precision`: fraction of retrieved alignment/gap chunks that human raters confirm relevant; validates the hybrid retriever and post-classification filtering.
- `Context Recall`: ensure critical alignment/gap evidence from ground-truth sets is included; flags missed retrieval or misclassification.
- `Context Relevancy (semantic similarity)`: cosine similarity between query and supplied alignment/gap contexts to guard against noisy or neutral chunks slipping through.
- `Context Entity Recall`: track required entities (company name, stage, geography, ARR) in retrieved evidence to confirm classifier surfaced the right signals.

**Generation Quality**
- `Hallucination Score (LLM judge)`: check explanations cite only evidence supplied in the correct section; catches cross-contamination between alignment/gap.
- `Answer Relevance (LLM judge)`: verify every alignment/gap bullet ties directly to VC criteria and plan details instead of generalities.
- `Answer Correctness (LLM judge + human spot-check)`: confirm quoted snippets are literal matches and interpretations are factually consistent.

**Agentic / Workflow Metrics**
- `Task Success Rate`: percent of runs producing ‚â•2 validated alignments and ‚â•2 validated gaps (targets from pipeline thresholds).
- `Routing Accuracy`: alignment vs gap classification agreement between automated labels and human review (e.g., confusion matrix, macro F1).
- `Latency`: overall query time plus per-stage breakdown (retrieval vs classification vs generation) against the 8‚Äì12‚ÄØs budget.
- `Cost per Query`: track API spend (classification vs generation) to ensure it stays within the $0.012 prototype target.

**Operational Checks**
- `Schema Validation`: automated JSON schema tests to guarantee downstream apps don‚Äôt break.
- `Smoke Test Pass Rate`: maintain the existing 5‚Äëcase suite as a continuous regression gate.

Together these cover Iteration‚Äë1‚Äôs priorities: accurate alignment/gap differentiation, grounded citations, acceptable latency/cost, and stable structured outputs.

---

_**User (2025-11-06 15:12-0800)**_

Now look at the code for Iteration 2 especially @reflection.py  and @research.py @analyzer.py @graph.py  and generate Evaluation metrics for Iteration 2

---

_**Agent (model default, mode Agent)**_

<think><details><summary>Thought Process</summary>
**Listing evaluation metrics**</details></think>

---

## Iteration‚Äë2 Evaluation Metrics

**Initial Analysis (carry over from Iteration‚ÄØ1)**
- `Exact Match / Schema Validation`: `AlignmentResponse` shape plus new fields (`overall_confidence`, `research_conducted`, `data_gaps`) when present.
- `Context Precision & Recall`: pre- and post-reflection retrieval sets still need relevance checks to ensure baseline quality.
- `Citation Guardrails`: hallucination, answer relevance, correctness judges ensuring regenerated output stays grounded.

**Reflection Layer**
- `Confidence Calibration`: compare LLM confidence scores vs. human rubric; track MAE and calibration buckets (e.g., strong/medium/weak vs. reviewed strength).
- `Claim Coverage`: percentage of alignment/gap claims receiving confidence scores; failure indicates parsing or reflection errors.
- `Gap Detection Precision/Recall`: confusion matrix of detected `InformationGap` items vs. reviewer judgments; focus on false negatives (missed gaps) and false positives (spurious research triggers).
- `Reflection Latency & Cost`: time/cost per reflection pass (ChatOpenAI 0.0 temp) to watch cascading overhead.

**Research Layer**
- `Baseline Query Success`: hit rate where baseline (`market`, `competitor`, `VC criteria`) queries return ‚â•1 usable snippet; monitors Serper availability and parsing.
- `Gap-Driven Query Yield`: fraction of gap-driven searches producing at least one snippet tied to the originating gap; evaluates `generate_research_queries`.
- `Research Coverage`: ratio of identified gaps that receive at least one validated research result before budget exhausted.
- `Budget Compliance`: ensure total searches ‚â§ `max_queries` (3 baseline + 2 gap) per run; flag overruns.
- `Result Precision`: reviewer validation of whether collected snippets actually address the gap (precision of `parse_search_results`).

**Integrated Workflow**
- `Overall Confidence Delta`: difference between Iteration‚Äë1 baseline confidence (implicit 0.7) and Iteration‚Äë2 reported `overall_confidence`; large drops or unexplained spikes trigger investigation.
- `Iterative Improvement Rate`: percentage of runs where research adds new sources or raises confidence above threshold; validates reflection‚Üíresearch loop effectiveness.
- `End-to-End Latency`: track total pipeline time with breakdown (initial analysis, reflection, research, regeneration) against acceptable UX budget.
- `Failure Modes`: monitor for `error` field population, missing `research_conducted`, or empty final explanations; categorize and trend.

These metrics collectively validate that Iteration‚ÄØ2‚Äôs reflection and research agents genuinely improve evidence quality, stay within time/cost budgets, and avoid injecting noisy or irrelevant data.

---

_**User (2025-11-06 15:12-0800)**_

Create an md file with both Iteration 1 and Iteration 2 Eval metrics

---

_**Agent (model default, mode Agent)**_

<think><details><summary>Thought Process</summary>
**Preparing to create new markdown file**</details></think>

---

<tool-use data-tool-type="write" data-tool-name="edit_file">

<details><summary>Tool use: **code_edit** ‚Ä¢ Edit file: /Users/pankaj/projects/vira/docs/Iteration Evaluation Metrics.md</summary>

**Chunk 1**
Lines added: 46, lines removed: 1

```diff
@@ -1,1 +1,46 @@
- 
+ # VIRA Evaluation Metrics Summary
+ 
+ ## Iteration 1 (Classification-Enhanced Retrieval)
+ 
+ - **Response Quality**
+   - Exact Match / Contains for JSON schema validation, company name, citation URLs.
+   - Regex checks enforcing URL domains (&#96;a16z.com&#96;) and required quote syntax (&#96;&quot;VC Criterion:&quot;&#96;, &#96;&quot;Business Plan:&quot;&#96;).
+   - BLEU/ROUGE and BERTScore spot checks against curated references to catch drift in alignment/gap explanations.
+ - **Retrieval Quality**
+   - Context Precision: share of retrieved alignment/gap chunks confirmed relevant by reviewers.
+   - Context Recall: percentage of mandatory evidence from ground-truth sets present in retrieval outputs.
+   - Context Relevancy: cosine similarity between queries and alignment/gap contexts to detect noisy inclusions.
+   - Context Entity Recall: coverage of critical entities (company, stage, geography, ARR) in retrieved evidence.
+ - **Generation Quality**
+   - Hallucination Score (LLM judge): ensures explanations cite only provided alignment/gap evidence.
+   - Answer Relevance (LLM judge): verifies each bullet addresses VC criteria and plan details directly.
+   - Answer Correctness (LLM judge + human QA): checks quotes and interpretations remain factually consistent.
+ - **Agentic / Workflow Metrics**
+   - Task Success Rate: runs producing ‚â•2 validated alignments and ‚â•2 validated gaps.
+   - Routing Accuracy: agreement between automated alignment/gap classification and human review (confusion matrix, macro F1).
+   - Latency: overall response plus stage-level timing (retrieval, classification, generation) against 8‚Äì12 s budget.
+   - Cost per Query: track classification vs. generation spend staying near $0.012 prototype target.
+ - **Operational Checks**
+   - Schema Validation: automated JSON schema tests for downstream stability.
+   - Smoke Test Pass Rate: maintain 5-case regression suite as gate.
+ 
+ ## Iteration 2 (Reflection &amp; Research Agents)
+ 
+ - **Initial Analysis (Baseline Continuity)**
+   - Exact Match / Schema Validation, Context Precision &amp; Recall, and citation guardrail metrics carried forward from Iteration 1.
+ - **Reflection Layer**
+   - Confidence Calibration: mean absolute error between LLM confidence scores and reviewer rubric (strong/medium/weak/insufficient).
+   - Claim Coverage: percentage of alignment/gap claims receiving confidence scores without parsing failures.
+   - Gap Detection Precision/Recall: confusion matrix of detected &#96;InformationGap&#96; items vs. reviewer judgements.
+   - Reflection Latency &amp; Cost: monitor per-pass runtime and API spend for the temperature-0 reflection model.
+ - **Research Layer**
+   - Baseline Query Success: rate at which the three mandatory queries return ‚â•1 usable snippet.
+   - Gap-Driven Query Yield: fraction of supplemental searches producing evidence tied to originating gaps.
+   - Research Coverage: ratio of identified gaps covered by at least one validated research result before budget exhaustion.
+   - Budget Compliance: ensure total executed searches respect configured limits (3 baseline + up to 2 gap-driven).
+   - Result Precision: reviewer confirmation that collected snippets address the intended gap.
+ - **Integrated Workflow**
+   - Overall Confidence Delta: difference between baseline (Iteration 1) confidence and reported Iteration 2 &#96;overall_confidence&#96;.
+   - Iterative Improvement Rate: share of runs where research adds new sources or raises confidence above threshold.
+   - End-to-End Latency: full pipeline timing breakdown (analysis, reflection, research, regeneration) vs. UX target.
+   - Failure Modes: track frequency of populated &#96;error&#96; fields, missing research artifacts, or empty final explanations.
```

</details>

</tool-use>

---

<think><details><summary>Thought Process</summary>
**Preparing concise final message**</details></think>

---

## Evaluation Metrics Doc Ready
- Added `docs/Iteration Evaluation Metrics.md` covering Iteration‚ÄØ1 metrics (response, retrieval, generation, workflow, ops) plus Iteration‚ÄØ2 additions (reflection, research, integrated checks).
- Structured for quick reference across teams; extends baseline metrics without duplicating existing docs.

Next: sanity-check against live telemetry or expand with owners once instrumentation lands.

---

_**User (2025-11-06 15:12-0800)**_

@Iter1RAG.md @Iter2Agents.md @VIRA_MVP_Architecture_Plan_v0.md Look at the mermaid diagrams for Iter1 and Iter2. See the Iteration3 proposed in the MVP architecture and create similar Mermaid diagram for Iter3Agents.md

---

_**Agent (model claude-4.5-sonnet-thinking, mode Agent)**_

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **code_edit** ‚Ä¢ Edit file: /Users/pankaj/projects/vira/Iter3Agents.md</summary>

**Chunk 1**
Lines added: 832, lines removed: 1

```diff
@@ -1,1 +1,832 @@
- 
+ # Iteration 3: Multi-Agent Investment Committee Architecture
+ 
+ This document visualizes the multi-agent architecture of VIRA&#39;s Iteration 3, which implements a full investment committee simulation with specialized agents, consensus synthesis, and proactive strategic recommendations.
+ 
+ ## üèóÔ∏è High-Level Architecture
+ 
+ &#96;&#96;&#96;mermaid
+ graph TB
+     subgraph &quot;User Input&quot;
+         A[Company Name + Business Plan + VC Criteria]
+     end
+     
+     subgraph &quot;Iteration 3: Multi-Agent Investment Committee&quot;
+         B[Coordinator Agent]
+         C[4 Specialist Agents]
+         D[Synthesis Agent]
+         E[Strategy Agent]
+         F[Dialogue Agent]
+     end
+     
+     subgraph &quot;Output&quot;
+         G[Multi-Perspective Investment Report]
+         H[Positioning Strategy + Benchmarks]
+         I[Interactive Q&amp;A Interface]
+     end
+     
+     A --&gt; B
+     B --&gt; C
+     C --&gt; D
+     D --&gt; E
+     E --&gt; G
+     E --&gt; H
+     F --&gt; I
+     
+     style B fill:#e1f5ff
+     style C fill:#fff4e1
+     style D fill:#e8f5e9
+     style E fill:#f3e5f5
+     style F fill:#fff9c4
+     style G fill:#c8e6c9
+     style H fill:#c8e6c9
+     style I fill:#c8e6c9
+ &#96;&#96;&#96;
+ 
+ ## üîÑ Multi-Agent Workflow
+ 
+ &#96;&#96;&#96;mermaid
+ graph TB
+     START([User Query]) --&gt; COORD[Coordinator Agent&lt;br/&gt;Task Planning]
+     
+     COORD --&gt; PARALLEL{Parallel Execution}
+     
+     PARALLEL --&gt; MARKET[Market Agent]
+     PARALLEL --&gt; PRODUCT[Product Agent]
+     PARALLEL --&gt; TEAM[Team Agent]
+     PARALLEL --&gt; FINANCE[Financial Agent]
+     
+     MARKET --&gt; |Market data&lt;br/&gt;Competition&lt;br/&gt;Timing| MEMORY[Shared Working Memory]
+     PRODUCT --&gt; |Tech moat&lt;br/&gt;Defensibility&lt;br/&gt;PMF signals| MEMORY
+     TEAM --&gt; |Founder exp&lt;br/&gt;Track record&lt;br/&gt;Domain expertise| MEMORY
+     FINANCE --&gt; |Unit econ&lt;br/&gt;Business model&lt;br/&gt;Capital efficiency| MEMORY
+     
+     MEMORY --&gt; SYNTH[Synthesis Agent&lt;br/&gt;Consensus + Disagreements]
+     
+     SYNTH --&gt; STRAT[Strategy Agent&lt;br/&gt;Positioning + Benchmarks]
+     
+     STRAT --&gt; DIAL{Interactive Mode?}
+     
+     DIAL --&gt; |Yes| DIALOGUE[Dialogue Agent&lt;br/&gt;Q&amp;A with Agents]
+     DIAL --&gt; |No| REPORT[Final Report]
+     
+     DIALOGUE --&gt; REPORT
+     
+     REPORT --&gt; END([Multi-Perspective Analysis])
+     
+     style COORD fill:#e3f2fd
+     style MARKET fill:#ffccbc
+     style PRODUCT fill:#c8e6c9
+     style TEAM fill:#f8bbd0
+     style FINANCE fill:#fff9c4
+     style MEMORY fill:#e0e0e0
+     style SYNTH fill:#ce93d8
+     style STRAT fill:#90caf9
+     style DIALOGUE fill:#fff59d
+     style REPORT fill:#81c784
+ &#96;&#96;&#96;
+ 
+ ## üéØ Specialist Agent Details
+ 
+ &#96;&#96;&#96;mermaid
+ graph LR
+     subgraph &quot;Market Agent&quot;
+         M1[TAM/SAM Analysis]
+         M2[Competitive Landscape]
+         M3[Market Timing]
+         M4[Growth Trends]
+         
+         M1 --&gt; MT[Market Tools]
+         M2 --&gt; MT
+         M3 --&gt; MT
+         M4 --&gt; MT
+         
+         MT --&gt; MC[Crunchbase API&lt;br/&gt;Web Search&lt;br/&gt;Industry Reports]
+     end
+     
+     subgraph &quot;Product Agent&quot;
+         P1[Tech Stack Analysis]
+         P2[Defensibility Assessment]
+         P3[PMF Signals]
+         P4[Innovation Score]
+         
+         P1 --&gt; PT[Product Tools]
+         P2 --&gt; PT
+         P3 --&gt; PT
+         P4 --&gt; PT
+         
+         PT --&gt; PC[GitHub API&lt;br/&gt;Patent Search&lt;br/&gt;Tech Blogs]
+     end
+     
+     subgraph &quot;Team Agent&quot;
+         T1[Founder Backgrounds]
+         T2[Domain Expertise]
+         T3[Prior Outcomes]
+         T4[Advisory Board]
+         
+         T1 --&gt; TT[Team Tools]
+         T2 --&gt; TT
+         T3 --&gt; TT
+         T4 --&gt; TT
+         
+         TT --&gt; TC[LinkedIn API&lt;br/&gt;Crunchbase&lt;br/&gt;News Search]
+     end
+     
+     subgraph &quot;Financial Agent&quot;
+         F1[Unit Economics]
+         F2[Business Model]
+         F3[Capital Efficiency]
+         F4[Growth Metrics]
+         
+         F1 --&gt; FT[Finance Tools]
+         F2 --&gt; FT
+         F3 --&gt; FT
+         F4 --&gt; FT
+         
+         FT --&gt; FC[Financial DBs&lt;br/&gt;Benchmark Data&lt;br/&gt;Model Parsers]
+     end
+     
+     style M1 fill:#ffccbc
+     style P1 fill:#c8e6c9
+     style T1 fill:#f8bbd0
+     style F1 fill:#fff9c4
+ &#96;&#96;&#96;
+ 
+ ## üß† Coordinator Agent Orchestration
+ 
+ &#96;&#96;&#96;mermaid
+ sequenceDiagram
+     participant U as User
+     participant C as Coordinator Agent
+     participant M as Market Agent
+     participant P as Product Agent
+     participant T as Team Agent
+     participant F as Financial Agent
+     participant SM as Shared Memory
+     participant SY as Synthesis Agent
+     
+     U-&gt;&gt;C: Business Plan + VC Criteria
+     
+     Note over C: Task Planning &amp; Dispatch
+     C-&gt;&gt;C: Analyze complexity
+     C-&gt;&gt;C: Assign research budgets
+     
+     par Parallel Execution (20-30s)
+         C-&gt;&gt;M: Analyze market fit
+         C-&gt;&gt;P: Analyze product fit
+         C-&gt;&gt;T: Analyze team fit
+         C-&gt;&gt;F: Analyze financial fit
+     end
+     
+     Note over M,F: Each agent conducts autonomous research
+     
+     M-&gt;&gt;SM: Store market findings
+     P-&gt;&gt;SM: Store product findings
+     T-&gt;&gt;SM: Store team findings
+     F-&gt;&gt;SM: Store financial findings
+     
+     Note over SM: Cross-Agent Communication
+     M-&gt;&gt;P: &quot;Found patent focus in competitors&quot;
+     P-&gt;&gt;M: &quot;Checked - no patents filed&quot;
+     
+     SM-&gt;&gt;SY: All agent perspectives
+     
+     Note over SY: Identify consensus + disagreements
+     SY-&gt;&gt;SY: Map strengths &amp; risks
+     SY-&gt;&gt;SY: Prioritize findings
+     
+     SY--&gt;&gt;U: Multi-Perspective Report
+ &#96;&#96;&#96;
+ 
+ ## üìä Shared Working Memory
+ 
+ &#96;&#96;&#96;mermaid
+ graph TB
+     subgraph &quot;Shared Memory Architecture&quot;
+         A[Vector Memory&lt;br/&gt;Embeddings of findings]
+         B[Key-Value Store&lt;br/&gt;Structured data]
+         C[Agent Notes&lt;br/&gt;Cross-agent messages]
+         D[Research Cache&lt;br/&gt;Avoid redundant searches]
+     end
+     
+     subgraph &quot;Agent Interactions&quot;
+         E[Agent 1 writes findings]
+         F[Agent 2 reads context]
+         G[Agent 3 requests info]
+         H[Agent 4 supplements]
+     end
+     
+     E --&gt; A
+     E --&gt; B
+     F --&gt; A
+     F --&gt; B
+     G --&gt; C
+     H --&gt; C
+     
+     A --&gt; D
+     B --&gt; D
+     C --&gt; D
+     
+     style A fill:#e3f2fd
+     style B fill:#fff9c4
+     style C fill:#c8e6c9
+     style D fill:#f8bbd0
+ &#96;&#96;&#96;
+ 
+ ## üîç Inter-Agent Communication
+ 
+ &#96;&#96;&#96;mermaid
+ flowchart LR
+     subgraph &quot;Communication Patterns&quot;
+         A[Request Info]
+         B[Share Finding]
+         C[Ask Opinion]
+         D[Verify Claim]
+     end
+     
+     subgraph &quot;Example Flow&quot;
+         M[Market Agent:&lt;br/&gt;&quot;Found 3 competitors&lt;br/&gt;with patents&quot;]
+         
+         M --&gt; |Request| P[Product Agent:&lt;br/&gt;Check our patent status]
+         
+         P --&gt; |Share| M[Product Agent:&lt;br/&gt;0 granted, 1 provisional]
+         
+         M --&gt; |Ask| F[Financial Agent:&lt;br/&gt;Impact on valuation?]
+         
+         F --&gt; |Verify| M[Financial Agent:&lt;br/&gt;IP moat worth 20-30%&lt;br/&gt;premium typically]
+     end
+     
+     A --&gt; M
+     B --&gt; P
+     C --&gt; F
+     
+     style M fill:#ffccbc
+     style P fill:#c8e6c9
+     style F fill:#fff9c4
+ &#96;&#96;&#96;
+ 
+ ## üé≠ Synthesis Agent Logic
+ 
+ &#96;&#96;&#96;mermaid
+ flowchart TD
+     INPUT[4 Agent Perspectives] --&gt; EXTRACT[Extract Assessments]
+     
+     EXTRACT --&gt; CONSENSUS{Identify Consensus}
+     
+     CONSENSUS --&gt; |3-4 agents agree| STRONG[Strong Consensus&lt;br/&gt;High confidence claims]
+     CONSENSUS --&gt; |2 agents agree| MODERATE[Moderate Consensus&lt;br/&gt;Medium confidence]
+     CONSENSUS --&gt; |1-1-1-1 split| DISAGREE[Disagreement Area&lt;br/&gt;Flag for human review]
+     
+     STRONG --&gt; RISK[Risk Prioritization]
+     MODERATE --&gt; RISK
+     DISAGREE --&gt; RISK
+     
+     RISK --&gt; RANK[Rank by:&lt;br/&gt;1. Impact&lt;br/&gt;2. Confidence&lt;br/&gt;3. Actionability]
+     
+     RANK --&gt; OUTPUT[Synthesis Report:&lt;br/&gt;Consensus + Disagreements + Risks]
+     
+     style CONSENSUS fill:#fff9c4
+     style STRONG fill:#4caf50
+     style MODERATE fill:#ffeb3b
+     style DISAGREE fill:#ff9800
+     style OUTPUT fill:#e8f5e9
+ &#96;&#96;&#96;
+ 
+ ### Consensus Algorithm
+ 
+ &#96;&#96;&#96;mermaid
+ graph TB
+     subgraph &quot;Agreement Scoring&quot;
+         A[&quot;4/4 agents agree&lt;br/&gt;‚Üí Strong Consensus ‚úì‚úì‚úì&quot;]
+         B[&quot;3/4 agents agree&lt;br/&gt;‚Üí Majority View ‚úì‚úì&quot;]
+         C[&quot;2/2 split&lt;br/&gt;‚Üí Requires Resolution ‚ö†&quot;]
+         D[&quot;No clear pattern&lt;br/&gt;‚Üí Human Judgment Needed ?&quot;]
+     end
+     
+     subgraph &quot;Conflict Resolution&quot;
+         E[Check evidence strength]
+         F[Weight by agent confidence]
+         G[Defer to domain expert agent]
+         H[Flag for user decision]
+     end
+     
+     C --&gt; E
+     C --&gt; F
+     C --&gt; G
+     D --&gt; H
+     
+     style A fill:#4caf50
+     style B fill:#8bc34a
+     style C fill:#ff9800
+     style D fill:#f44336
+ &#96;&#96;&#96;
+ 
+ ## üí° Strategy Agent Components
+ 
+ &#96;&#96;&#96;mermaid
+ graph TB
+     subgraph &quot;Strategy Agent Inputs&quot;
+         A[Synthesis Report]
+         B[VC Portfolio Data]
+         C[Comparable Companies]
+     end
+     
+     subgraph &quot;Strategy Generation&quot;
+         D[Positioning for Startup]
+         E[Positioning for VC]
+         F[Benchmark Analysis]
+         G[Risk Mitigation]
+     end
+     
+     subgraph &quot;Outputs&quot;
+         H[Strengths to Emphasize]
+         I[Gaps to Address]
+         J[Target Partner Match]
+         K[Comparable Positioning]
+         L[Action Items]
+     end
+     
+     A --&gt; D
+     A --&gt; E
+     B --&gt; E
+     C --&gt; F
+     
+     D --&gt; H
+     D --&gt; I
+     E --&gt; J
+     F --&gt; K
+     D --&gt; L
+     E --&gt; L
+     
+     style D fill:#e3f2fd
+     style E fill:#c8e6c9
+     style F fill:#fff9c4
+     style G fill:#ffccbc
+ &#96;&#96;&#96;
+ 
+ ### Positioning Strategy Flow
+ 
+ &#96;&#96;&#96;mermaid
+ flowchart TD
+     START[Synthesis Results] --&gt; STRENGTH{Identify Top 3&lt;br/&gt;Strengths}
+     
+     STRENGTH --&gt; S1[Strength 1:&lt;br/&gt;How to emphasize]
+     STRENGTH --&gt; S2[Strength 2:&lt;br/&gt;How to emphasize]
+     STRENGTH --&gt; S3[Strength 3:&lt;br/&gt;How to emphasize]
+     
+     START --&gt; GAPS{Identify Top 3&lt;br/&gt;Gaps}
+     
+     GAPS --&gt; G1[Gap 1:&lt;br/&gt;Mitigation tactic]
+     GAPS --&gt; G2[Gap 2:&lt;br/&gt;Mitigation tactic]
+     GAPS --&gt; G3[Gap 3:&lt;br/&gt;Mitigation tactic]
+     
+     S1 --&gt; MATCH[Match to Portfolio&lt;br/&gt;Companies]
+     S2 --&gt; MATCH
+     S3 --&gt; MATCH
+     
+     MATCH --&gt; PARTNER[Identify Target&lt;br/&gt;Partner]
+     
+     G1 --&gt; ACTIONS[Prioritized&lt;br/&gt;Action Items]
+     G2 --&gt; ACTIONS
+     G3 --&gt; ACTIONS
+     
+     PARTNER --&gt; OUTPUT[Positioning&lt;br/&gt;Strategy]
+     ACTIONS --&gt; OUTPUT
+     
+     style STRENGTH fill:#c8e6c9
+     style GAPS fill:#ffccbc
+     style MATCH fill:#e3f2fd
+     style OUTPUT fill:#4caf50
+ &#96;&#96;&#96;
+ 
+ ## üí¨ Dialogue Agent Interface
+ 
+ &#96;&#96;&#96;mermaid
+ sequenceDiagram
+     participant U as User
+     participant D as Dialogue Agent
+     participant R as Router
+     participant A as Specialist Agent
+     participant M as Memory
+     
+     U-&gt;&gt;D: &quot;Why do you think the product is weak?&quot;
+     
+     D-&gt;&gt;R: Route query to relevant agent
+     R-&gt;&gt;R: Analyze question type
+     
+     Note over R: Product-related ‚Üí Product Agent
+     
+     R-&gt;&gt;A: Forward to Product Agent
+     
+     A-&gt;&gt;M: Retrieve original assessment
+     A-&gt;&gt;A: Generate detailed explanation
+     
+     A--&gt;&gt;D: &quot;The plan mentions AI but lacks:&lt;br/&gt;1. Architecture details&lt;br/&gt;2. Data moat explanation&lt;br/&gt;3. Competitive differentiation&quot;
+     
+     D--&gt;&gt;U: Formatted response with context
+     
+     U-&gt;&gt;D: &quot;What would improve the score?&quot;
+     
+     D-&gt;&gt;A: Follow-up query
+     A-&gt;&gt;A: Generate recommendations
+     
+     A--&gt;&gt;D: &quot;3 improvements:&lt;br/&gt;1. File provisional patent&lt;br/&gt;2. Show network effects timeline&lt;br/&gt;3. Demonstrate data advantage&quot;
+     
+     D--&gt;&gt;U: Actionable recommendations
+ &#96;&#96;&#96;
+ 
+ ### Query Routing Logic
+ 
+ &#96;&#96;&#96;mermaid
+ flowchart TD
+     QUERY[User Question] --&gt; CLASSIFY{Classify Question Type}
+     
+     CLASSIFY --&gt; |Market/Competition| MARKET[Route to Market Agent]
+     CLASSIFY --&gt; |Product/Tech| PRODUCT[Route to Product Agent]
+     CLASSIFY --&gt; |Team/Founders| TEAM[Route to Team Agent]
+     CLASSIFY --&gt; |Financials/Metrics| FINANCE[Route to Financial Agent]
+     CLASSIFY --&gt; |Overall/Synthesis| SYNTH[Route to Synthesis Agent]
+     CLASSIFY --&gt; |Strategy/Positioning| STRAT[Route to Strategy Agent]
+     
+     MARKET --&gt; RESPOND[Generate Response]
+     PRODUCT --&gt; RESPOND
+     TEAM --&gt; RESPOND
+     FINANCE --&gt; RESPOND
+     SYNTH --&gt; RESPOND
+     STRAT --&gt; RESPOND
+     
+     RESPOND --&gt; CONTEXT[Add Context&lt;br/&gt;from Memory]
+     CONTEXT --&gt; OUTPUT[Formatted Answer]
+     
+     style CLASSIFY fill:#fff9c4
+     style RESPOND fill:#c8e6c9
+     style OUTPUT fill:#4caf50
+ &#96;&#96;&#96;
+ 
+ ## üìà Agent Research Workflows
+ 
+ ### Market Agent Research Flow
+ 
+ &#96;&#96;&#96;mermaid
+ flowchart LR
+     START[Business Plan&lt;br/&gt;Market Section] --&gt; Q1[Generate TAM Query]
+     START --&gt; Q2[Generate Competitor Query]
+     START --&gt; Q3[Generate Timing Query]
+     
+     Q1 --&gt; |Crunchbase API| R1[Market Size Data]
+     Q2 --&gt; |Web Search| R2[Competitor List + Funding]
+     Q3 --&gt; |Google Trends| R3[Market Interest Signals]
+     
+     R1 --&gt; ANALYZE[Market Analysis]
+     R2 --&gt; ANALYZE
+     R3 --&gt; ANALYZE
+     
+     ANALYZE --&gt; CONF{Confidence&lt;br/&gt;Check}
+     
+     CONF --&gt; |High &gt;80%| DONE[Complete]
+     CONF --&gt; |Low &lt;60%| DEEP[Deep Dive&lt;br/&gt;2-3 more searches]
+     
+     DEEP --&gt; ANALYZE
+     
+     style START fill:#ffccbc
+     style ANALYZE fill:#fff9c4
+     style DONE fill:#4caf50
+ &#96;&#96;&#96;
+ 
+ ### Team Agent Research Flow
+ 
+ &#96;&#96;&#96;mermaid
+ flowchart LR
+     START[Business Plan&lt;br/&gt;Team Section] --&gt; EXTRACT[Extract Founder Names]
+     
+     EXTRACT --&gt; L1[LinkedIn Search:&lt;br/&gt;CEO Background]
+     EXTRACT --&gt; L2[LinkedIn Search:&lt;br/&gt;CTO Background]
+     
+     L1 --&gt; |LinkedIn API| E1[Experience Data]
+     L2 --&gt; |LinkedIn API| E2[Experience Data]
+     
+     E1 --&gt; CHECK{Domain&lt;br/&gt;Expertise?}
+     E2 --&gt; CHECK
+     
+     CHECK --&gt; |Yes| PRIOR[Search Prior&lt;br/&gt;Company Outcomes]
+     CHECK --&gt; |No| FLAG[Flag as Risk]
+     
+     PRIOR --&gt; |Crunchbase| OUTCOMES[Exit Data]
+     
+     OUTCOMES --&gt; ASSESS[Team Assessment]
+     FLAG --&gt; ASSESS
+     
+     style START fill:#f8bbd0
+     style CHECK fill:#fff9c4
+     style ASSESS fill:#4caf50
+ &#96;&#96;&#96;
+ 
+ ## üéØ Benchmark Comparison Engine
+ 
+ &#96;&#96;&#96;mermaid
+ graph TB
+     subgraph &quot;Input Data&quot;
+         A[Target Company Metrics]
+         B[VC Portfolio Companies]
+         C[Market Comparables]
+     end
+     
+     subgraph &quot;Comparison Dimensions&quot;
+         D[Market Size Percentile]
+         E[Team Experience Percentile]
+         F[Capital Efficiency Percentile]
+         G[Tech Moat Percentile]
+     end
+     
+     subgraph &quot;Benchmark Report&quot;
+         H[Ranking vs Portfolio]
+         I[Strengths Relative to Peers]
+         J[Gaps Relative to Peers]
+         K[Closest Comparable Match]
+     end
+     
+     A --&gt; D
+     A --&gt; E
+     A --&gt; F
+     A --&gt; G
+     
+     B --&gt; D
+     B --&gt; E
+     B --&gt; F
+     B --&gt; G
+     
+     C --&gt; D
+     C --&gt; F
+     
+     D --&gt; H
+     E --&gt; H
+     F --&gt; H
+     G --&gt; H
+     
+     H --&gt; I
+     H --&gt; J
+     H --&gt; K
+     
+     style D fill:#e3f2fd
+     style H fill:#c8e6c9
+     style K fill:#4caf50
+ &#96;&#96;&#96;
+ 
+ ### Percentile Calculation
+ 
+ &#96;&#96;&#96;mermaid
+ flowchart TD
+     METRIC[Company Metric:&lt;br/&gt;e.g., Market Size] --&gt; GATHER[Gather Portfolio&lt;br/&gt;Company Data]
+     
+     GATHER --&gt; SORT[Sort in&lt;br/&gt;Ascending Order]
+     
+     SORT --&gt; POSITION[Find Position&lt;br/&gt;of Target Company]
+     
+     POSITION --&gt; CALC[Calculate:&lt;br/&gt;Percentile = Position/Total √ó 100]
+     
+     CALC --&gt; INTERPRET{Interpret Result}
+     
+     INTERPRET --&gt; |&gt;75%| TOP[Top Quartile&lt;br/&gt;Strong Signal ‚úì‚úì‚úì]
+     INTERPRET --&gt; |50-75%| ABOVE[Above Average&lt;br/&gt;Positive Signal ‚úì‚úì]
+     INTERPRET --&gt; |25-50%| BELOW[Below Average&lt;br/&gt;Concern ‚ö†]
+     INTERPRET --&gt; |&lt;25%| BOTTOM[Bottom Quartile&lt;br/&gt;Red Flag ‚úó]
+     
+     style CALC fill:#fff9c4
+     style TOP fill:#4caf50
+     style ABOVE fill:#8bc34a
+     style BELOW fill:#ff9800
+     style BOTTOM fill:#f44336
+ &#96;&#96;&#96;
+ 
+ ## üìä Output Report Structure
+ 
+ &#96;&#96;&#96;mermaid
+ classDiagram
+     class InvestmentCommitteeReport {
+         +str company_name
+         +str vc_firm
+         +List~AgentAssessment~ agent_perspectives
+         +SynthesisResult synthesis
+         +PositioningStrategy strategy
+         +DialogueInterface dialogue
+     }
+     
+     class AgentAssessment {
+         +str agent_name
+         +List~Finding~ findings
+         +float overall_confidence
+         +List~Risk~ risks_identified
+     }
+     
+     class Finding {
+         +str category
+         +str claim
+         +str evidence
+         +str confidence_grade
+     }
+     
+     class SynthesisResult {
+         +List~str~ consensus_strengths
+         +List~Disagreement~ disagreement_areas
+         +List~PrioritizedRisk~ top_risks
+     }
+     
+     class Disagreement {
+         +str topic
+         +Dict~str,str~ agent_opinions
+         +str resolution_approach
+     }
+     
+     class PositioningStrategy {
+         +List~str~ startup_tactics
+         +List~str~ vc_fit_analysis
+         +BenchmarkData benchmarks
+         +str target_partner
+     }
+     
+     class BenchmarkData {
+         +Dict~str,float~ percentiles
+         +str closest_comparable
+         +List~str~ portfolio_matches
+     }
+     
+     InvestmentCommitteeReport --&gt; &quot;4&quot; AgentAssessment
+     InvestmentCommitteeReport --&gt; SynthesisResult
+     InvestmentCommitteeReport --&gt; PositioningStrategy
+     AgentAssessment --&gt; &quot;*&quot; Finding
+     SynthesisResult --&gt; &quot;*&quot; Disagreement
+     PositioningStrategy --&gt; BenchmarkData
+     
+     style InvestmentCommitteeReport fill:#e3f2fd
+     style AgentAssessment fill:#fff9c4
+     style SynthesisResult fill:#ce93d8
+     style PositioningStrategy fill:#90caf9
+ &#96;&#96;&#96;
+ 
+ ## ‚öôÔ∏è Configuration &amp; Control
+ 
+ &#96;&#96;&#96;mermaid
+ graph TB
+     subgraph &quot;Configuration Settings&quot;
+         A[ENABLE_ITERATION_3&lt;br/&gt;true/false]
+         B[MAX_TOOL_CALLS_PER_AGENT&lt;br/&gt;default: 5]
+         C[TOTAL_TOOL_BUDGET&lt;br/&gt;default: 20]
+         D[CONSENSUS_THRESHOLD&lt;br/&gt;default: 0.75]
+         E[PARALLEL_EXECUTION&lt;br/&gt;default: true]
+     end
+     
+     subgraph &quot;Runtime Behavior&quot;
+         F{ENABLE_ITERATION_3?}
+         F --&gt;|true| G[Use Multi-Agent&lt;br/&gt;Committee System]
+         F --&gt;|false| H[Fall back to&lt;br/&gt;Iteration 2]
+     end
+     
+     subgraph &quot;Agent Control&quot;
+         I[Research Budget&lt;br/&gt;per Agent]
+         J[Confidence&lt;br/&gt;Thresholds]
+         K[Iteration Limits]
+     end
+     
+     A --&gt; F
+     B --&gt; I
+     C --&gt; I
+     D --&gt; J
+     E --&gt; G
+     
+     G --&gt; I
+     G --&gt; J
+     G --&gt; K
+     
+     style A fill:#4caf50
+     style G fill:#e3f2fd
+     style I fill:#fff9c4
+ &#96;&#96;&#96;
+ 
+ ## üîß Implementation Files
+ 
+ | Component | File | Key Functions/Classes |
+ |-----------|------|----------------------|
+ | **Orchestration** | &#96;src/vira/agents/committee/coordinator.py&#96; | &#96;CoordinatorAgent&#96;, &#96;dispatch_parallel()&#96; |
+ | **Specialist Agents** | &#96;src/vira/agents/committee/specialists.py&#96; | &#96;MarketAgent&#96;, &#96;ProductAgent&#96;, &#96;TeamAgent&#96;, &#96;FinancialAgent&#96; |
+ | **Shared Memory** | &#96;src/vira/agents/committee/memory.py&#96; | &#96;SharedMemory&#96;, &#96;store_finding()&#96;, &#96;retrieve_context()&#96; |
+ | **Synthesis** | &#96;src/vira/agents/committee/synthesis.py&#96; | &#96;SynthesisAgent&#96;, &#96;identify_consensus()&#96;, &#96;resolve_conflicts()&#96; |
+ | **Strategy** | &#96;src/vira/agents/committee/strategy.py&#96; | &#96;StrategyAgent&#96;, &#96;generate_positioning()&#96;, &#96;benchmark_analysis()&#96; |
+ | **Dialogue** | &#96;src/vira/agents/committee/dialogue.py&#96; | &#96;DialogueAgent&#96;, &#96;route_query()&#96;, &#96;interactive_qa()&#96; |
+ | **State** | &#96;src/vira/agents/committee/state.py&#96; | &#96;CommitteeState&#96;, &#96;AgentAssessment&#96;, &#96;SynthesisResult&#96; |
+ | **API Integration** | &#96;src/vira/backend/api.py&#96; | &#96;/analyze&#96; endpoint (Iteration 3 mode) |
+ 
+ ## üìà Performance Characteristics
+ 
+ &#96;&#96;&#96;mermaid
+ graph TB
+     subgraph &quot;Iteration 2: Reflective Agent&quot;
+         I2A[Query Time: ~15-30s]
+         I2B[API Calls: 5-10]
+         I2C[Cost: $0.05-0.15]
+         I2D[Single Perspective]
+     end
+     
+     subgraph &quot;Iteration 3: Multi-Agent Committee&quot;
+         I3A[Query Time: ~20-40s]
+         I3B[API Calls: 15-25]
+         I3C[Cost: $0.30-0.80]
+         I3D[4 Perspectives + Synthesis]
+     end
+     
+     subgraph &quot;Value Add&quot;
+         V1[Multi-dimensional Analysis]
+         V2[Consensus Identification]
+         V3[Strategic Recommendations]
+         V4[Benchmark Comparisons]
+         V5[Interactive Q&amp;A]
+         V6[Investment Committee Grade]
+     end
+     
+     I3A --&gt; V1
+     I3B --&gt; V2
+     I3C --&gt; V3
+     I3D --&gt; V4
+     I3D --&gt; V5
+     
+     V1 --&gt; GRADE[75% Match with&lt;br/&gt;Human Committees]
+     V2 --&gt; GRADE
+     V3 --&gt; GRADE
+     V4 --&gt; GRADE
+     V5 --&gt; GRADE
+     V6 --&gt; GRADE
+     
+     style I3A fill:#fff9c4
+     style I3C fill:#ffccbc
+     style GRADE fill:#4caf50
+     style V1 fill:#e3f2fd
+ &#96;&#96;&#96;
+ 
+ ## üéØ Key Innovations
+ 
+ 1. **Parallel Agent Execution**: 4 agents work simultaneously (20-30s vs 60-80s sequential)
+ 2. **Shared Working Memory**: Cross-agent communication and context sharing
+ 3. **Consensus Synthesis**: Automatic identification of agreement vs. disagreement
+ 4. **Multi-Perspective Analysis**: Investment committee simulation with specialist views
+ 5. **Strategic Positioning**: Actionable recommendations for both startups and VCs
+ 6. **Benchmark Engine**: Percentile ranking vs. portfolio and market comparables
+ 7. **Interactive Dialogue**: Q&amp;A with individual agents for deeper exploration
+ 8. **Adaptive Research**: Agents autonomously decide research depth based on confidence
+ 
+ ## üÜö Iteration Comparison
+ 
+ &#96;&#96;&#96;mermaid
+ graph TB
+     subgraph &quot;Iteration 1: Basic RAG&quot;
+         I1A[Single Pass]
+         I1B[No Confidence]
+         I1C[No Research]
+         I1D[Explanation Only]
+     end
+     
+     subgraph &quot;Iteration 2: Reflective Agent&quot;
+         I2A[Reflection Loop]
+         I2B[Confidence Scoring]
+         I2C[Gap-Filling Research]
+         I2D[Enhanced Explanation]
+     end
+     
+     subgraph &quot;Iteration 3: Multi-Agent Committee&quot;
+         I3A[Parallel Agents]
+         I3B[Consensus Synthesis]
+         I3C[Strategic Positioning]
+         I3D[Committee-Grade Report]
+     end
+     
+     I1A -.Evolves to.-&gt; I2A
+     I1B -.Evolves to.-&gt; I2B
+     I1C -.Evolves to.-&gt; I2C
+     I1D -.Evolves to.-&gt; I2D
+     
+     I2A -.Evolves to.-&gt; I3A
+     I2B -.Evolves to.-&gt; I3B
+     I2C -.Evolves to.-&gt; I3C
+     I2D -.Evolves to.-&gt; I3D
+     
+     style I1A fill:#e0e0e0
+     style I1B fill:#e0e0e0
+     style I1C fill:#e0e0e0
+     style I1D fill:#e0e0e0
+     
+     style I2A fill:#e3f2fd
+     style I2B fill:#e3f2fd
+     style I2C fill:#e3f2fd
+     style I2D fill:#e3f2fd
+     
+     style I3A fill:#c8e6c9
+     style I3B fill:#c8e6c9
+     style I3C fill:#c8e6c9
+     style I3D fill:#c8e6c9
+ &#96;&#96;&#96;
+ 
+ ---
+ 
+ **See also:**
+ - &#96;Iter1RAG.md&#96; - Iteration 1 RAG architecture diagrams
+ - &#96;Iter2Agents.md&#96; - Iteration 2 reflective agent diagrams
+ - &#96;VIRA_MVP_Architecture_Plan_v0.md&#96; (lines 733-1116) - Original Iteration 3 spec
+ 
+ 
```

</details>

</tool-use>

---

Perfect! I've created `Iter3Agents.md` with comprehensive Mermaid diagrams for the Iteration 3 multi-agent investment committee architecture. The document includes:

**Key Diagrams:**
- High-level architecture showing the 6-agent system (Coordinator, 4 Specialists, Synthesis, Strategy, Dialogue)
- Multi-agent workflow with parallel execution
- Specialist agent details (Market, Product, Team, Financial) with their tools
- Coordinator orchestration sequence diagram
- Shared working memory architecture
- Inter-agent communication patterns
- Synthesis agent logic with consensus algorithm
- Strategy agent components and positioning flow
- Dialogue agent interface with query routing
- Agent research workflows (Market & Team examples)
- Benchmark comparison engine with percentile calculations
- Output report class structure
- Configuration and control flow
- Performance comparison vs Iteration 2
- Evolution from Iteration 1 ‚Üí 2 ‚Üí 3

The diagrams follow the same visual style and structure as Iter1RAG.md and Iter2Agents.md, making it easy to compare the three iterations and understand the progressive complexity and value add at each stage.

---

_**User (2025-11-06 15:12-0800)**_

Generate 4 bullet points summarizing the essential features added per Iteration according to the plan @VIRA_MVP_Architecture_Plan_v0.md 

---

_**Agent (model claude-4.5-sonnet-thinking, mode Agent)**_

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** ‚Ä¢ Read file: /Users/pankaj/projects/vira/VIRA_MVP_Architecture_Plan_v0.md</summary>

</details>
</tool-use>

---

Based on the VIRA MVP Architecture Plan, here are the essential features added per iteration:

## **Iteration 1: Explanation-Only System (RAG)**
- **Hybrid Retrieval System**: Semantic + BM25 keyword search (70/30 weight) to extract relevant VC criteria from vectorstore
- **Structured Explanation Generator**: Produces balanced alignment matches and gaps analysis without scoring or recommendations
- **Trust-First Design**: Transparent evidence-backed claims with source URLs; system provides analysis, users make decisions
- **Fast Criteria Surfacing**: Single-pass RAG pipeline generating structured JSON output in <5 seconds

## **Iteration 2: Reflective Agent with Gap-Filling Research**
- **Reflection Agent**: Meta-assessment of explanation quality with claim-level confidence scoring (‚úì‚úì‚úì strong ‚Üí ? insufficient)
- **Autonomous Research Agent**: Proactive web search (max 5 queries) to fill identified information gaps via Serper/Exa APIs
- **Self-Critique Loop**: System identifies limitations, triggers research when confidence <0.7, regenerates with enhanced context
- **Evidence Grading System**: Confidence quantification per claim with transparency notes distinguishing verified facts from assumptions

## **Iteration 3: Multi-Agent Investment Committee with Proactive Strategy**
- **Multi-Agent System**: 4 specialized agents (Market, Product, Team, Financial) working in parallel with shared memory
- **Investment Committee Synthesis**: Consensus identification, disagreement mapping, and risk prioritization across agent perspectives
- **Proactive Positioning Strategy**: Actionable recommendations for both startups (how to strengthen fit) and VCs (strategic fit analysis)
- **Interactive Dialogue Mode**: Q&A capability to query individual agents, benchmark comparisons vs portfolio companies, comparable analysis

---

_**User (2025-11-06 15:12-0800)**_

Generate Evaluation Metrics that seem apt for Iteration 3 and build on Iteration 1 and 2

---

_**Agent (model claude-4.5-sonnet-thinking, mode Agent)**_

Based on Iterations 1 and 2 evaluation metrics, here are the appropriate metrics for Iteration 3:

## Iteration 3 Evaluation Metrics

**Multi-Agent Coordination**
- `Agent Task Completion Rate`: Percentage of specialist agents successfully completing their assigned analysis tasks within allocated tool budget.
- `Parallel Execution Efficiency`: Actual vs. theoretical speedup from parallel agent execution (target: 2.5-3x faster than sequential).
- `Cross-Agent Communication Quality`: Relevance and utility of inter-agent messages/requests as judged by human reviewers.
- `Shared Memory Utilization`: Percentage of agent findings that are referenced/used by other agents, indicating effective knowledge sharing.

**Consensus & Synthesis Quality**
- `Consensus Accuracy`: Agreement between AI-identified consensus areas (3-4 agents agree) and human expert panels on same cases.
- `Disagreement Detection Precision/Recall`: How accurately the synthesis agent identifies genuine expert disagreements vs. false positives.
- `Risk Prioritization Correlation`: Spearman correlation between AI-ranked risks and human analyst risk rankings.
- `Synthesis Completeness`: Coverage of key investment dimensions (market, product, team, financial) in final report; target 100% coverage.

**Strategic Positioning Quality**
- `Recommendation Actionability`: Percentage of strategic recommendations that are specific, measurable, and implementable (human review).
- `Recommendation Adoption Rate`: Percentage of startups/VCs who actually use positioning tactics in real outreach (tracked via user surveys).
- `Benchmark Accuracy`: MAE between AI-calculated percentiles and human-verified portfolio rankings across key metrics.
- `Partner Match Precision`: Accuracy of target partner identification validated against actual portfolio fit patterns.

**Interactive Dialogue Performance**
- `Query Routing Accuracy`: Percentage of user questions correctly routed to the most appropriate specialist agent.
- `Answer Completeness`: Human rating of whether dialogue responses fully address user questions (1-5 scale, target >4.0).
- `Follow-up Reduction`: Percentage decrease in user follow-up questions compared to Iteration 2 single-perspective output.
- `Agent Perspective Clarity`: User rating of how well individual agent viewpoints are differentiated and explained.

**Committee-Grade Output Quality**
- `Human Committee Agreement`: Percentage agreement between multi-agent assessment and actual human investment committee decisions (target: 75%+).
- `Perspective Diversity`: Cosine dissimilarity between agent outputs, ensuring genuinely distinct viewpoints (target: >0.3 pairwise).
- `Decision Support Value`: User rating of report's usefulness for investment decisions (1-5 scale, target >4.2).
- `Comparative Analysis Quality`: Accuracy of comparable company matches and portfolio benchmarks validated by domain experts.

**Integrated Workflow (Building on Iter 1 & 2)**
- `End-to-End Latency with Parallelization`: Total pipeline time with agent parallelization vs. budgeted 20-40s; breakdown by orchestration/research/synthesis stages.
- `Tool Call Budget Compliance`: Adherence to 20-call total budget (5 per agent avg) with variance tracking and overage detection.
- `Cost per Committee Analysis`: Total API spend for full multi-agent run staying within $0.30-0.80 target range.
- `Confidence Calibration Across Agents`: Whether per-agent confidence scores remain well-calibrated (MAE vs. human agreement per agent type).

**Operational & Quality Controls**
- `Schema Validation`: Automated checks ensuring `InvestmentCommitteeReport`, `AgentAssessment`, and `SynthesisResult` structures are complete and valid.
- `Agent Failure Handling`: Percentage of runs successfully recovering from individual agent failures without full pipeline abort.
- `Memory Consistency`: Validation that shared working memory maintains consistency across concurrent agent writes.

These metrics collectively validate that Iteration 3's multi-agent committee delivers investment-grade analysis, maintains diverse perspectives, provides actionable strategy, and operates within time/cost budgets while building on the foundation of retrieval quality (Iter 1) and reflection/research capabilities (Iter 2).

---

_**User (2025-11-06 15:12-0800)**_

Add these into the @Iteration1n2 Evaluation Metrics.md and rename as Iterations123 EvalMetrics.md

---

_**Agent (model claude-4.5-sonnet-thinking, mode Agent)**_

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** ‚Ä¢ Read file: /Users/pankaj/projects/vira/evaldocs/Iteration1n2 Evaluation Metrics.md</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **code_edit** ‚Ä¢ Edit file: /Users/pankaj/projects/vira/evaldocs/Iterations123 EvalMetrics.md</summary>

**Chunk 1**
Lines added: 84, lines removed: 1

```diff
@@ -1,1 +1,84 @@
- 
+ # VIRA Evaluation Metrics Summary
+ 
+ ## Iteration 1 (Classification-Enhanced Retrieval)
+ 
+ - **Response Quality**
+   - Exact Match / Contains for JSON schema validation, company name, citation URLs.
+   - Regex checks enforcing URL domains (&#96;a16z.com&#96;) and required quote syntax (&#96;&quot;VC Criterion:&quot;&#96;, &#96;&quot;Business Plan:&quot;&#96;).
+   - BLEU/ROUGE and BERTScore spot checks against curated references to catch drift in alignment/gap explanations.
+ - **Retrieval Quality**
+   - Context Precision: share of retrieved alignment/gap chunks confirmed relevant by reviewers.
+   - Context Recall: percentage of mandatory evidence from ground-truth sets present in retrieval outputs.
+   - Context Relevancy: cosine similarity between queries and alignment/gap contexts to detect noisy inclusions.
+   - Context Entity Recall: coverage of critical entities (company, stage, geography, ARR) in retrieved evidence.
+ - **Generation Quality**
+   - Hallucination Score (LLM judge): ensures explanations cite only provided alignment/gap evidence.
+   - Answer Relevance (LLM judge): verifies each bullet addresses VC criteria and plan details directly.
+   - Answer Correctness (LLM judge + human QA): checks quotes and interpretations remain factually consistent.
+ - **Agentic / Workflow Metrics**
+   - Task Success Rate: runs producing ‚â•2 validated alignments and ‚â•2 validated gaps.
+   - Routing Accuracy: agreement between automated alignment/gap classification and human review (confusion matrix, macro F1).
+   - Latency: overall response plus stage-level timing (retrieval, classification, generation) against 8‚Äì12 s budget.
+   - Cost per Query: track classification vs. generation spend staying near $0.012 prototype target.
+ - **Operational Checks**
+   - Schema Validation: automated JSON schema tests for downstream stability.
+   - Smoke Test Pass Rate: maintain 5-case regression suite as gate.
+ 
+ ## Iteration 2 (Reflection &amp; Research Agents)
+ 
+ - **Initial Analysis (Baseline Continuity)**
+   - Exact Match / Schema Validation, Context Precision &amp; Recall, and citation guardrail metrics carried forward from Iteration 1.
+ - **Reflection Layer**
+   - Confidence Calibration: mean absolute error between LLM confidence scores and reviewer rubric (strong/medium/weak/insufficient).
+   - Claim Coverage: percentage of alignment/gap claims receiving confidence scores without parsing failures.
+   - Gap Detection Precision/Recall: confusion matrix of detected &#96;InformationGap&#96; items vs. reviewer judgements.
+   - Reflection Latency &amp; Cost: monitor per-pass runtime and API spend for the temperature-0 reflection model.
+ - **Research Layer**
+   - Baseline Query Success: rate at which the three mandatory queries return ‚â•1 usable snippet.
+   - Gap-Driven Query Yield: fraction of supplemental searches producing evidence tied to originating gaps.
+   - Research Coverage: ratio of identified gaps covered by at least one validated research result before budget exhaustion.
+   - Budget Compliance: ensure total executed searches respect configured limits (3 baseline + up to 2 gap-driven).
+   - Result Precision: reviewer confirmation that collected snippets address the intended gap.
+ - **Integrated Workflow**
+   - Overall Confidence Delta: difference between baseline (Iteration 1) confidence and reported Iteration 2 &#96;overall_confidence&#96;.
+   - Iterative Improvement Rate: share of runs where research adds new sources or raises confidence above threshold.
+   - End-to-End Latency: full pipeline timing breakdown (analysis, reflection, research, regeneration) vs. UX target.
+   - Failure Modes: track frequency of populated &#96;error&#96; fields, missing research artifacts, or empty final explanations.
+ 
+ ## Iteration 3 (Multi-Agent Investment Committee)
+ 
+ - **Multi-Agent Coordination**
+   - Agent Task Completion Rate: percentage of specialist agents successfully completing their assigned analysis tasks within allocated tool budget.
+   - Parallel Execution Efficiency: actual vs. theoretical speedup from parallel agent execution (target: 2.5-3x faster than sequential).
+   - Cross-Agent Communication Quality: relevance and utility of inter-agent messages/requests as judged by human reviewers.
+   - Shared Memory Utilization: percentage of agent findings that are referenced/used by other agents, indicating effective knowledge sharing.
+ - **Consensus &amp; Synthesis Quality**
+   - Consensus Accuracy: agreement between AI-identified consensus areas (3-4 agents agree) and human expert panels on same cases.
+   - Disagreement Detection Precision/Recall: how accurately the synthesis agent identifies genuine expert disagreements vs. false positives.
+   - Risk Prioritization Correlation: Spearman correlation between AI-ranked risks and human analyst risk rankings.
+   - Synthesis Completeness: coverage of key investment dimensions (market, product, team, financial) in final report; target 100% coverage.
+ - **Strategic Positioning Quality**
+   - Recommendation Actionability: percentage of strategic recommendations that are specific, measurable, and implementable (human review).
+   - Recommendation Adoption Rate: percentage of startups/VCs who actually use positioning tactics in real outreach (tracked via user surveys).
+   - Benchmark Accuracy: MAE between AI-calculated percentiles and human-verified portfolio rankings across key metrics.
+   - Partner Match Precision: accuracy of target partner identification validated against actual portfolio fit patterns.
+ - **Interactive Dialogue Performance**
+   - Query Routing Accuracy: percentage of user questions correctly routed to the most appropriate specialist agent.
+   - Answer Completeness: human rating of whether dialogue responses fully address user questions (1-5 scale, target &gt;4.0).
+   - Follow-up Reduction: percentage decrease in user follow-up questions compared to Iteration 2 single-perspective output.
+   - Agent Perspective Clarity: user rating of how well individual agent viewpoints are differentiated and explained.
+ - **Committee-Grade Output Quality**
+   - Human Committee Agreement: percentage agreement between multi-agent assessment and actual human investment committee decisions (target: 75%+).
+   - Perspective Diversity: cosine dissimilarity between agent outputs, ensuring genuinely distinct viewpoints (target: &gt;0.3 pairwise).
+   - Decision Support Value: user rating of report&#39;s usefulness for investment decisions (1-5 scale, target &gt;4.2).
+   - Comparative Analysis Quality: accuracy of comparable company matches and portfolio benchmarks validated by domain experts.
+ - **Integrated Workflow (Building on Iter 1 &amp; 2)**
+   - End-to-End Latency with Parallelization: total pipeline time with agent parallelization vs. budgeted 20-40s; breakdown by orchestration/research/synthesis stages.
+   - Tool Call Budget Compliance: adherence to 20-call total budget (5 per agent avg) with variance tracking and overage detection.
+   - Cost per Committee Analysis: total API spend for full multi-agent run staying within $0.30-0.80 target range.
+   - Confidence Calibration Across Agents: whether per-agent confidence scores remain well-calibrated (MAE vs. human agreement per agent type).
+ - **Operational &amp; Quality Controls**
+   - Schema Validation: automated checks ensuring &#96;InvestmentCommitteeReport&#96;, &#96;AgentAssessment&#96;, and &#96;SynthesisResult&#96; structures are complete and valid.
+   - Agent Failure Handling: percentage of runs successfully recovering from individual agent failures without full pipeline abort.
+   - Memory Consistency: validation that shared working memory maintains consistency across concurrent agent writes.
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="delete_file">
<details><summary>Tool use: **delete_file**</summary>

Explanation: Removing the old file after creating the new comprehensive version with all three iterations.


</details>
</tool-use>

---

Perfect! I've created the comprehensive `Iterations123 EvalMetrics.md` file that includes evaluation metrics for all three iterations:

**What was added:**

1. **Iteration 1 & 2 metrics** - Carried forward from the original file
2. **Iteration 3 metrics** - Added six new categories:
   - Multi-Agent Coordination (task completion, parallel efficiency, communication quality, memory utilization)
   - Consensus & Synthesis Quality (consensus accuracy, disagreement detection, risk prioritization, completeness)
   - Strategic Positioning Quality (recommendation actionability, adoption rate, benchmark accuracy, partner matching)
   - Interactive Dialogue Performance (routing accuracy, answer completeness, follow-up reduction, perspective clarity)
   - Committee-Grade Output Quality (human committee agreement, perspective diversity, decision support value, comparative analysis)
   - Integrated Workflow & Operational Controls (latency with parallelization, tool budget compliance, cost tracking, schema validation, failure handling)

The old `Iteration1n2 Evaluation Metrics.md` file has been removed and replaced with the new comprehensive version covering all three iterations.

---

